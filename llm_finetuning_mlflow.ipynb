{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.225.0  datasets==2.18.0 transformers==4.40.0 mlflow==2.13.2 sagemaker-mlflow==0.1.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from steps.finetune_llama8b_hf import finetune_llama8b\n",
    "from steps.preprocess_llama3 import preprocess\n",
    "from steps.eval_mlflow import evaluation\n",
    "from steps.utils import create_training_job_name\n",
    "import os\n",
    "\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SageMaker Session & IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Couldn't call 'get_role' to get Role ARN from role name AppUser to get Role path.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(role)\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    # Hard coded ARN since, I'm running this notebook locally to reduce AWS costs\n",
    "    role = \"arn:aws:iam::891612587330:role/service-role/AmazonSageMaker-ExecutionRole-20241230T123665\"\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"experiment_name\": \"all_target_modules_1K\",\n",
    "    \"model_id\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"model_version\": \"3.0.2\",\n",
    "    \"model_name\": \"llama-3-8b\",\n",
    "    \"endpoint_name\": \"llama-3-8b\",\n",
    "    \"finetune_instance_type\": \"ml.g5.12xlarge\",\n",
    "    \"finetune_num_instances\": 1,\n",
    "    \"instance_type\": \"ml.g5.12xlarge\",\n",
    "    \"num_instances\": 1,\n",
    "    \"epoch\": 1,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = {\"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_arn = \"arn:aws:sagemaker:us-east-2:891612587330:mlflow-tracking-server/llm-finetuning-experiment\"  # fill MLflow tracking server ARN\n",
    "experiment_name = \"llm-finetuning-experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"HuggingFaceH4/no_robots\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = ParameterString(name=\"lora_config\", default_value=json.dumps(lora_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline name is training-evaulation-pipeline-mlflow\n",
      "Output S3 bucket: s3://sagemaker-us-east-1-711387114394/datasets/hf_no_robots/output_training-evaulation-pipeline-mlflow\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = \"training-evaulation-pipeline-mlflow\"\n",
    "\n",
    "default_bucket = sagemaker.Session().default_bucket()\n",
    "main_data_path = f\"s3://{default_bucket}\"\n",
    "evaluation_data_path = (\n",
    "    main_data_path\n",
    "    + \"/datasets/hf_no_robots/evaluation/automatic_small/dataset_evaluation_small.jsonl\"\n",
    ")\n",
    "output_data_path = main_data_path + \"/datasets/hf_no_robots/output_\" + pipeline_name\n",
    "\n",
    "# You can add your own evaluation dataset code into this step\n",
    "preprocess_step_ret = step(preprocess, name=\"preprocess\")(\n",
    "    default_bucket,\n",
    "    dataset_name,\n",
    "    train_sample=100,\n",
    "    eval_sample=100,\n",
    "    mlflow_arn=mlflow_arn,\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")\n",
    "\n",
    "print(\"The pipeline name is \" + pipeline_name)\n",
    "# Mark the name of this bucket for reviewing the artifacts generated by this pipeline at the end of the execution\n",
    "print(\"Output S3 bucket: \" + output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finetune_llama7b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m finetune_ret \u001b[38;5;241m=\u001b[39m step(finetune_llama7b, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetune_llama8b_instruction\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[1;32m      2\u001b[0m     preprocess_step_ret,\n\u001b[1;32m      3\u001b[0m     train_config,\n\u001b[1;32m      4\u001b[0m     lora_config,\n\u001b[1;32m      5\u001b[0m     role,\n\u001b[1;32m      6\u001b[0m     mlflow_arn,\n\u001b[1;32m      7\u001b[0m     experiment_name,\n\u001b[1;32m      8\u001b[0m     ExecutionVariables\u001b[38;5;241m.\u001b[39mPIPELINE_EXECUTION_ID,\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'finetune_llama7b' is not defined"
     ]
    }
   ],
   "source": [
    "finetune_ret = step(finetune_llama8b, name=\"finetune_llama8b_instruction\")(\n",
    "    preprocess_step_ret,\n",
    "    train_config,\n",
    "    lora_config,\n",
    "    role,\n",
    "    mlflow_arn,\n",
    "    experiment_name,\n",
    "    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
